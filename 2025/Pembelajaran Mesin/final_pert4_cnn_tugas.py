# -*- coding: utf-8 -*-
"""FINAL Pert4-CNN-Tugas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10r5AZTBj2Jiqt2kon_waQUs1nPt7fKJS

#Mengambil Data dari Kaggle
"""

!pip install kaggle
!pip install tensorflow
!pip install matplotlib

from google.colab import files
files.upload()

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

print('Train images shape:', x_train.shape)
print('Train labels shape:', y_train.shape)

print('Test images shape:', x_test.shape)
print('Test labels shape:', y_test.shape)

import tensorflow as tf
import matplotlib.pyplot as plt

(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()

# Tampilkan beberapa gambar dari dataset MNIST
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
axes = axes.ravel()

for i in range(10):
    axes[i].imshow(x_train[i], cmap='gray')
    axes[i].set_title(f'Label: {y_train[i]}')
    axes[i].axis('off')

plt.tight_layout()
plt.show()

"""#Import Library"""

!pip install kaggle
!pip install tensorflow
!pip install idx2numpy

"""#Preprocessing Data"""

import numpy as np

"""##Missing Value Check"""

def check_missing_values(data, name):
    missing_values = np.isnan(data).sum()
    print(f"Missing values in {name}:", missing_values)

check_missing_values(x_train, "x_train")
check_missing_values(y_train, "y_train")
check_missing_values(x_test, "x_test")
check_missing_values(y_test, "y_test")

"""##Duplicate Data Check"""

def check_duplicates(data, name):
    unique_data = np.unique(data, axis=0)
    duplicates = data.shape[0] - unique_data.shape[0]
    print(f"Duplicates in {name}:", duplicates)

check_duplicates(x_train, "x_train")
check_duplicates(y_train, "y_train")
check_duplicates(x_test, "x_test")
check_duplicates(y_test, "y_test")

import numpy as np

unique_labels, label_counts = np.unique(y_train, return_counts=True)

print("Unique labels:", unique_labels)
print("Count per label:", label_counts)

"""##Normalisasi Data"""

x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

"""##Reshaping Data"""

x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

"""##One-hot Encoding Label"""

y_train_cat = tf.keras.utils.to_categorical(y_train, 10)
y_test_cat = tf.keras.utils.to_categorical(y_test, 10)

fig, axes = plt.subplots(2, 5, figsize=(12, 5))
axes = axes.ravel()

for i in range(10):
    axes[i].imshow(x_train[i].squeeze(), cmap='gray')
    axes[i].set_title(f'Label: {y_train[i]}')
    axes[i].axis('off')

plt.tight_layout()
plt.show()

"""#Pembagungan, Pelatihan, dan Perbandingan Model

##Percobaan 1
"""

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Callbacks
ES = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=2, restore_best_weights=True, mode='max', min_delta=0)
MC = tf.keras.callbacks.ModelCheckpoint(filepath='Best_model.keras', monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')
RP = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, min_lr=0.0001, factor=0.2)

# Compile Model
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001))

"""### Evaluasi Model 1"""

history = model.fit(x_train, y_train_cat, validation_data=(x_test, y_test_cat), epochs=50, callbacks=[ES, MC, RP])

# Plot Training & Validation Accuracy and Loss dalam 1 Baris
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].plot(history.history['accuracy'], 'r', label='train')
axes[0].plot(history.history['val_accuracy'], 'b', label='val')
axes[0].set_title('Training and Validation Accuracy')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()

axes[1].plot(history.history['loss'], 'r', label='train')
axes[1].plot(history.history['val_loss'], 'b', label='val')
axes[1].set_title('Training and Validation Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()

plt.tight_layout()
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

predictions = model.predict(x_test)
y_pred = np.argmax(predictions, axis=1)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)



"""##Percobaan 2"""

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)

"""### Evaluasi Model 2"""

history = model.fit(x_train, y_train_cat, epochs=50, validation_data=(x_test, y_test_cat), batch_size=128, callbacks=[early_stop])

# Plot Training & Validation Accuracy and Loss dalam 1 Baris
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].plot(history.history['accuracy'], 'r', label='train')
axes[0].plot(history.history['val_accuracy'], 'b', label='val')
axes[0].set_title('Training and Validation Accuracy')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()

axes[1].plot(history.history['loss'], 'r', label='train')
axes[1].plot(history.history['val_loss'], 'b', label='val')
axes[1].set_title('Training and Validation Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()

plt.tight_layout()
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

predictions = model.predict(x_test)
y_pred = np.argmax(predictions, axis=1)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)



"""##Percobaan 3"""

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Conv2D(128, kernel_size=(3,3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Callbacks
ES = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=2, restore_best_weights=True, mode='max', min_delta=0)
MC = tf.keras.callbacks.ModelCheckpoint(filepath='Best_model.keras', monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')
RP = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=2, min_lr=0.0001, factor=0.2)

model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))

"""### Evaluasi Model 3"""

history = model.fit(x_train, y_train_cat, validation_data=(x_test, y_test_cat), epochs=50, batch_size=128, callbacks=[ES, MC, RP])

# Plot Training & Validation Accuracy and Loss dalam 1 Baris
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].plot(history.history['accuracy'], 'r', label='train')
axes[0].plot(history.history['val_accuracy'], 'b', label='val')
axes[0].set_title('Training and Validation Accuracy')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()

axes[1].plot(history.history['loss'], 'r', label='train')
axes[1].plot(history.history['val_loss'], 'b', label='val')
axes[1].set_title('Training and Validation Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()

plt.tight_layout()
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

predictions = model.predict(x_test)
y_pred = np.argmax(predictions, axis=1)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

accuracy = accuracy_score(y_test, y_pred)
print(f'Overall Accuracy: {accuracy * 100:.2f}%')